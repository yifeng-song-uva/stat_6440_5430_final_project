{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c064465-1463-42ba-80dc-5835fe1e6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from text_processing_utils import *\n",
    "import glob\n",
    "import math\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d79fc90-8c61-4229-862f-33995aa21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ratings = np.array(pickle.load(open(\"data/scaledata/cleaned_ratings.pickle\", \"rb\")))\n",
    "cleaned_reviews = pickle.load(open(\"data/scaledata/cleaned_reviews.pickle\", \"rb\"))\n",
    "vocabulary_dict = pickle.load(open(\"data/scaledata/vocabulary_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ddd8d24-7f82-4d25-b823-e790274b0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004 1002\n"
     ]
    }
   ],
   "source": [
    "# randomly split the movie reviews data into training/testing parts (80:20)\n",
    "np.random.seed(7654321)\n",
    "train_indices = np.random.choice(np.arange(len(cleaned_ratings)), int(len(cleaned_ratings)*0.8), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(cleaned_ratings)), train_indices)\n",
    "print(len(train_indices), len(test_indices))\n",
    "train_bow = convert_bow([cleaned_reviews[i] for i in train_indices])\n",
    "test_bow = convert_bow([cleaned_reviews[i] for i in test_indices])\n",
    "train_y = cleaned_ratings[train_indices]\n",
    "test_y = cleaned_ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40149f13-a0bc-4142-bed6-960add115585",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 36 # number of topics\n",
    "V = len(vocabulary_dict) # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "561f59c7-95a9-450d-9604-9478ba22011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the optimized global parameters from the model training phase\n",
    "np.random.seed(12345)\n",
    "new_alpha = pickle.load(open(\"data/scaledata/K_{}/alpha.pickle\".format(K), \"rb\"))\n",
    "new_xi = pickle.load(open(\"data/scaledata/K_{}/xi.pickle\".format(K), \"rb\"))\n",
    "new_eta = pickle.load(open(\"data/scaledata/K_{}/eta.pickle\".format(K), \"rb\"))\n",
    "new_delta = pickle.load(open(\"data/scaledata/K_{}/delta.pickle\".format(K), \"rb\"))\n",
    "new_Lambda = pickle.load(open(\"data/scaledata/K_{}/Lambda.pickle\".format(K), \"rb\"))\n",
    "input_data_x = test_bow\n",
    "input_data_y = test_y\n",
    "fpath = \"fragmented_output_files/\" # where to store the temporary fragmented files during parallelized E steps\n",
    "if not os.path.exists(fpath[:-1]):\n",
    "    os.makedirs(fpath[:-1])\n",
    "else:\n",
    "    for fn in glob.glob(fpath + \"*\"):\n",
    "        os.remove(fn)\n",
    "epsilon = 1e-4 # stopping criteria for convergence in E step\n",
    "predict = True # prediction mode for evaluating the test set\n",
    "\n",
    "## Run batch mode variational EM\n",
    "elbo_vs_time = [-math.inf]\n",
    "improve_in_elbo = math.inf\n",
    "time_elapsed = 0\n",
    "    \n",
    "## Run one iteration of unsupervised E step (parallelized) to identify the optimal local variational parameters for all documents in the test set.\n",
    "## The optimized phi identified from the unsupervised batch VI will be used to generate the predictions for response variable y in the test set\n",
    "%run -i \"parallelized_sLDA_E_step.py\" # unsupervised batch VI for sLDA is way faster than supervised one\n",
    "all_gamma = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"gamma*\")]\n",
    "new_gamma_dict = merge_dict(all_gamma)\n",
    "new_gamma = create_gamma_matrix(new_gamma_dict)\n",
    "all_phi = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"phi*\")]\n",
    "new_phi = merge_dict(all_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90cdd765-3c3a-4772-a12e-23d27b57b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['emotional', 'art', 'visual', 'producers', 'despite', 'however', 'nearly', 'color', 'aspect', 'deep'] 1.31036044964282\n",
      "Topic 2: ['movies', 'humor', 'why', 'ever', 'then', 'sex', 'comes', 'sexual', 'gives', 'old'] 0.3611204456296484\n",
      "Topic 3: ['line', 'subject', 'men', 'style', 'subscribe', 'details', 'women', 'find', 'word', 'attempt'] 0.7674899959713208\n",
      "Topic 4: ['seemed', 'clear', 'plenty', 'williams', 'fairly', 'chris', 'parker', 'kiss', 'creepy', 'terribly'] -0.686860373922249\n",
      "Topic 5: ['through', 'where', 'feels', 'narrative', 'stories', 'yet', 'adult', 'mark', 'times', 'modern'] 0.9095796544160848\n",
      "Topic 6: ['right', 'half', 'actually', 'watching', 'lines', 'works', 'going', 'along', 'believe', 'whose'] 0.4034043438733903\n",
      "Topic 7: ['kids', 'son', 'pg', 'jeffrey', '10', 'laugh', 'thought', 'joke', 'call', 'bunch'] 0.5537137774476548\n",
      "Topic 8: ['de', 'rich', 'oscar', 'colors', 'force', 'today', 'images', 'contrast', 'road', 'intense'] 2.2689603737657578\n",
      "Topic 9: ['funny', 'plot', 'scene', 'since', 'almost', 'teenagers', 'seen', 'language', 'off', 've'] 0.28625329180153436\n",
      "Topic 10: ['high', 'game', 'school', 'woody', 'debut', 'joe', 'basketball', 'affection', 'chases', 'forrest'] 0.7385666058280442\n",
      "Topic 11: ['still', 'think', 'fun', 'anything', 'done', 'reason', '2', 'nice', 'manages', 'our'] -0.10345362303651484\n",
      "Topic 12: ['thing', 'family', 'heart', 'beauty', 'kevin', 'using', 'taken', 'tragedy', 'child', 'period'] 1.1107370231458091\n",
      "Topic 13: ['bad', 'play', 'least', 'interesting', 'sort', 'worse', 'guys', 'dull', 'came', 'fair'] -1.4671824648499374\n",
      "Topic 14: ['comedy', 'him', 'artist', 'lawrence', 'eddie', 'roberts', 'niro', 'cynical', 'gag', 'neil'] 0.12593297215068056\n",
      "Topic 15: ['man', 'comic', 'room', 'reply', 'situations', 'simply', 'screening', 'idea', 'himself', 'premise'] 0.023655268606340485\n",
      "Topic 16: ['here', 'ending', 'itself', 'effort', 'noir', 'shown', 'telling', 'felt', 'eyes', 'lot'] 0.6204545504388026\n",
      "Topic 17: ['set', 'watch', 'waste', 'sometimes', 'thriller', 'romance', 'opinions', 'second', 'let', 'completely'] 0.7631899634710244\n",
      "Topic 18: ['give', 'before', 'rather', 'said', 'bond', '9', 'alive', 'tried', 'whatever', 'images'] 0.5549800088788217\n",
      "Topic 19: ['year', 'long', 'acting', 'violence', 'great', 'end', 'must', 'go', 'fine', 'poor'] 0.328889145071674\n",
      "Topic 20: ['music', 'history', 'allen', 'shakespeare', 'bergman', 'festival', 'climax', 'joy', 'filmmaker', 'maker'] 1.451866954045144\n",
      "Topic 21: ['david', 'musical', 'leaves', 'animated', 'upon', 'adventure', 'reaction', 'number', 'america', 'amusing'] 0.47858006921118523\n",
      "Topic 22: ['re', 'both', 'real', 'role', 'shot', 'hand', 'used', 'same', 'stone', 'killer'] 0.5301680703514471\n",
      "Topic 23: ['really', 'world', 'say', 'over', 'minutes', 'less', 'sense', 'ca', 'given', 'cinematography'] 0.7475727372045213\n",
      "Topic 24: ['gets', 'potential', 'country', 'ideas', 'boy', 'paul', 'dog', 'jack', 'frank', 'god'] 0.9333436840330052\n",
      "Topic 25: ['john', 'special', 'effects', 'summer', 'villain', 'steven', 'familiar', 'pretty', 'science', 'awful'] 0.7241666619160778\n",
      "Topic 26: ['seems', 'every', 'point', 'always', 'message', 'running', 'michael', 'piece', 'years', 'elements'] 0.9578193053874244\n",
      "Topic 27: ['us', 'between', 'making', 'take', 'hollywood', 'dialogue', 'american', 'each', 'becomes', 'obvious'] 0.2159639184874642\n",
      "Topic 28: ['actors', 'last', 'r', 'star', 'back', 'bit', 'worth', 'full', 'looks', 'profanity'] 0.6181331741899925\n",
      "Topic 29: ['screen', 'far', 'new', 'often', 'viewers', 'themes', 'level', 'production', 'effective', 'hard'] 0.6840437876478891\n",
      "Topic 30: ['though', 'things', 'three', 'tale', 'experience', 'someone', 'dark', 'documentary', 'own', 'mr'] 0.6955441107974583\n",
      "Topic 31: ['love', 'disney', 's', 'features', 'adults', 'u', 'appealing', 'distributor', 'jones', 'delightful'] 1.0847146089336661\n",
      "Topic 32: ['big', 'excellent', 'totally', 'age', 'recommend', 'average', 'lot', 'mine', 'expressed', '0'] 1.1723507648859623\n",
      "Topic 33: ['know', 'horror', 'points', 'king', 'gay', 'scream', 'branagh', 'monster', 'ice', 'murphy'] 0.8297269221877848\n",
      "Topic 34: ['action', 'again', 'sequences', 'genre', 'however', 'example', 'nothing', 'dead', 'series', 'certain'] 0.011114253186127243\n",
      "Topic 35: ['human', 'different', 'title', 'viewer', 'difficult', 'taking', 'woman', 'dramatic', 'cinema', 'leave'] 1.4102452815010729\n",
      "Topic 36: ['original', 'romantic', 'simple', 'keep', 'likely', 'serious', 'entertainment', 'home', 'close', 'tv'] 0.6438605962397048\n"
     ]
    }
   ],
   "source": [
    "# Topic words of each topic learned from training sLDA, with corresponding coefficients (eta)\n",
    "inverse_vocabulary_dict = {v:k for k,v in vocabulary_dict.items()}\n",
    "for t in range(K):\n",
    "    word_indices = np.argsort(new_Lambda[t,:])[::-1][:10]\n",
    "    print(\"Topic {}:\".format(t+1), [inverse_vocabulary_dict[j] for j in word_indices], new_eta[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c45d5b0f-95b9-472c-a1bc-39f40f6d6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the response variable y for the test set:\n",
    "# For Gaussian response, y_hat = E[phi_bar^T eta]\n",
    "phi_bar = {k:v.mean(axis=0) for k,v in new_phi.items()}\n",
    "pred_y = np.empty((len(test_y),))\n",
    "for indx in range(len(test_y)):\n",
    "    pred_y[indx] = np.dot(phi_bar[indx], new_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "558d8248-ebd2-4fd2-ba76-56c96df57198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1869059803755514"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictive R^2\n",
    "def predictive_R2(test_y, pred_y):\n",
    "    return 1 - np.mean((test_y - pred_y)**2) / np.var(test_y)\n",
    "predictive_R2(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0382008-ca04-4b02-b4a7-4b60a609107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.43325864600685615, pvalue=4.1492091221226707e-47)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pearson correlation coefficient\n",
    "pearsonr(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ced15-82da-4ae0-8fb5-69c9e0b1b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
