{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b9d8e64-bdb3-4674-aba4-cf5bf964f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from variational_inference_utils import *\n",
    "from scipy.special import polygamma\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d32774-12f5-44f0-b52b-2f21a3e24c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI_sLDA_M_Step:\n",
    "    '''\n",
    "    The default mode is minibatch natural gradient descent\n",
    "    '''\n",
    "    def __init__(self, K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, rho=None, corpus_size=None):\n",
    "        self.K = K # number of topics\n",
    "        self.bow = bow # list of dictionaries, with length D\n",
    "        self.doc_len = [sum(list(v.values())) for v in bow] # number of words within each document\n",
    "        self.y = y # D-dimensional vector\n",
    "        self.alpha = alpha # K-dimensional vector\n",
    "        self.new_alpha = None\n",
    "        self.xi = xi # V-dimensional vector\n",
    "        self.new_xi = None\n",
    "        self.eta = eta\n",
    "        self.new_eta = None\n",
    "        self.delta = delta\n",
    "        self.new_delta = None\n",
    "        self.Lambda = Lambda # size: K x V\n",
    "        self.new_Lambda = None\n",
    "        self.D = len(self.bow) # batch_size: number of documents in the minibatch\n",
    "        self.gamma = gamma # size: D x K\n",
    "        self.phi = phi # for each document, size is N_d x K\n",
    "        self.phi_bar = np.vstack(self.phi[d].mean(axis=0) for d in range(self.D)) # size: D x K\n",
    "        self.expect_x_x_t = np.zeros(shape=(K,K)) # size: K x K\n",
    "        for d in range(self.D):\n",
    "            N_d = self.doc_len[d]\n",
    "            self.expect_x_x_t += 1/N_d**2 * (self.phi[d].T @ self.phi[d])\n",
    "            for i in self.doc_len[d]: # correct for E[Z_nZ_n^T]\n",
    "                self.expect_x_x_t = self.expect_x_x_t + 1/N_d**2 * (self.phi[d][i,:] - self.phi[d][i,:]**2) \n",
    "        self.rho = rho\n",
    "        self.corpus_size = corpus_size\n",
    "        \n",
    "    def update_Lambda(self):\n",
    "        # update rule for global variational parameter Lambda\n",
    "        Lambda_hat = np.zeros_like(self.Lambda) # natural gradient of ELBO w.r.t the variational distribution q(beta | Lambda)\n",
    "        for wi,v in enumerate(self.bow):\n",
    "            for d in range(self.D):\n",
    "                Lambda_hat[:,v] += self.phi[d][wi,:]\n",
    "            Lambda_hat[:,v] = self.corpus_size / self.D * Lambda_hat # scale based on minibatch size\n",
    "            Lambda_hat[:,v] += self.xi[v] \n",
    "        self.new_Lambda = stochastic_variational_update(self.Lambda, Lambda_hat, self.rho)\n",
    "        \n",
    "    def update_alpha(self, batch = False):\n",
    "        # update rule for the global hyperparameter alpha\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        g = self.D * (polygamma(0, alpha_sum) - polygamma(0, self.alpha)) # gradient of ELBO w.r.t. alpha\n",
    "        g += polygamma(0, self.gamma).sum(axis=0) - np.sum(polygamma(0, self.gamma.sum(axis=1)))\n",
    "        g = self.corpus_size / self.D * g # scale based on minibatch size\n",
    "        h = -self.corpus_size * polygamma(1, self.alpha)\n",
    "        z = self.corpus_size * polygamma(1, alpha_sum)\n",
    "        alpha_hat = linear_time_natural_gradient(g, h, z) # compute (the scaled) natural gradient of ELBO w.r.t. p(theta_{1:corpur_size} | alpha)\n",
    "        if batch == False:\n",
    "            self.new_alpha = stochastic_hyperparameter_update(self.alpha, alpha_hat, self.rho)\n",
    "        else:\n",
    "            self.new_alpha -= alpha_hat\n",
    "\n",
    "    def update_xi(self):\n",
    "        # update rule for the global hyperparameter alpha\n",
    "        xi_sum = np.sum(self.xi)\n",
    "        g = self.K * (polygamma(0, xi_sum) - polygamma(0, self.xi)) # gradient of ELBO w.r.t. xi\n",
    "        g += polygamma(0, self.Lambda).sum(axis=0) - np.sum(polygamma(0, self.Lambda.sum(axis=1)))\n",
    "        h = -self.K * polygamma(1, self.xi)\n",
    "        z = self.K * polygamma(1, xi_sum)\n",
    "        xi_hat = linear_time_natural_gradient(g, h, z) # compute natural gradient of ELBO w.r.t. p(beta_{1:K} | xi)\n",
    "        self.new_alpha = stochastic_hyperparameter_update(self.xi, xi_hat, self.rho)\n",
    "        \n",
    "    def update_eta_and_delta(self):\n",
    "        # update rule for the global hyperparameter eta (Gaussian response)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar) # K-dimensional vector\n",
    "        expect_x_x_t_times_eta = np.dot(self.expect_x_x_t, self.eta)\n",
    "        y_t_y = np.sum(self.y**2)\n",
    "        temp_var = np.sum(self.eta * (phi_bar_times_y - expect_x_x_t_times_eta/2))\n",
    "        g_eta = (1/self.delta)*(phi_bar_times_y - expect_x_x_t_times_eta)\n",
    "        g_delta = 1/2/self.delta + 1/2/self.delta^2*(y_t_y - 2*temp_var)\n",
    "        g = self.corpus_size / self.D * np.hstack(g_eta, np.array([g_delta])) # gradient is of K+1 dimensional, scale based on minibatch size\n",
    "        h_11 = -1/self.delta*self.expet_x_x_t\n",
    "        h_21 = -g_eta / self.delta\n",
    "        h_22 = -1/2/self.delta**2 - 1/self.delta**3 * (y_t_y - 2*temp_var)\n",
    "        h = np.zeros(shape=(self.K+1, self.K+1))\n",
    "        h[:self.K, :self.K] = h_11\n",
    "        h[self.K, self.K] = h_22\n",
    "        h[self.K, :self.K] = h_21\n",
    "        h[:self.K, self.K] = h_21\n",
    "        h = self.corpus_size / self.D * h # (scaled) Hessian is of (K+1) x (K+1) dimensional\n",
    "        h_inv = np.linalg.inv(h)\n",
    "        eta_delta_hat = h_inv @ g # approximated natural gradient of ELBO w.r.t P(Y_{1:corpus_size}|eta, delta)\n",
    "        updated_eta_delta = stochastic_hyperparameter_update(np.hstack(self.eta, np.array([self.delta]), eta_delta_hat, self.rho))\n",
    "        \n",
    "    def run(self):\n",
    "        # run the full M-step\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40108f13-715f-4731-8d7e-eb5f0a008321",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VI_sLDA_M_Step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mbatch_VI_sLDA_M_Step\u001b[39;00m(VI_sLDA_M_Step):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, K, bow, alpha, xi, eta, delta, Lambda, gamma, phi, epsilon): \n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(K, bow, alpha, xi, eta, delta, Lambda, gamma, phi)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VI_sLDA_M_Step' is not defined"
     ]
    }
   ],
   "source": [
    "class batch_VI_sLDA_M_Step(VI_sLDA_M_Step):\n",
    "\n",
    "    def __init__(self, K, bow, alpha, xi, eta, delta, Lambda, gamma, phi, epsilon): \n",
    "        super().__init__(K, bow, alpha, xi, eta, delta, Lambda, gamma, phi)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def optimize_alpha(self):\n",
    "        # run a full Newton-Raphson algorithm to optimize alpha in M step\n",
    "        change_in_alpha = math.inf\n",
    "        while chage_in_alpha > self.epsilon:\n",
    "            self.update_alpha(batch=True)\n",
    "            change_in_alpha = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.alpha = self.new_alpha\n",
    "\n",
    "    def update_xi(self):\n",
    "        # run a full Newton-Raphson algorithm to optimize xi in M step\n",
    "        change_in_xi = math.inf\n",
    "        while chage_in_xi > self.epsilon:\n",
    "            self.update_xi(batch=True)\n",
    "            change_in_xi = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.xi = self.new_xi\n",
    "\n",
    "    def optimize_eta_and_delta(self):\n",
    "        # optimize in terms of eta and delta has a closed-form solution in batch VI mode\n",
    "        expect_x_x_t_inv = np.linalg.inv(self.expect_x_x_t)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar)\n",
    "        self.eta_new = np.dot(expect_x_x_t_inv, phi_bar_times_y) \n",
    "        self.delta_new = 1/self.D * (np.sum(self.y**2) - np.sum(np.dot(phi_bar_times_y, self.eta_new)))\n",
    "\n",
    "    def compute_elbo(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        # override the .run() method in the parent Class\n",
    "        # run the full M step\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e06b0-2a02-4174-aa0b-f58f869a77a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
