{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c064465-1463-42ba-80dc-5835fe1e6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from text_processing_utils import *\n",
    "import glob\n",
    "import math\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d79fc90-8c61-4229-862f-33995aa21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ratings = np.array(pickle.load(open(\"data/scaledata/cleaned_ratings.pickle\", \"rb\")))\n",
    "cleaned_reviews = pickle.load(open(\"data/scaledata/cleaned_reviews.pickle\", \"rb\"))\n",
    "vocabulary_dict = pickle.load(open(\"data/scaledata/vocabulary_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ddd8d24-7f82-4d25-b823-e790274b0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004 1002\n"
     ]
    }
   ],
   "source": [
    "# randomly split the movie reviews data into training/testing parts (80:20)\n",
    "np.random.seed(7654321)\n",
    "train_indices = np.random.choice(np.arange(len(cleaned_ratings)), int(len(cleaned_ratings)*0.8), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(cleaned_ratings)), train_indices)\n",
    "print(len(train_indices), len(test_indices))\n",
    "train_bow = convert_bow([cleaned_reviews[i] for i in train_indices])\n",
    "test_bow = convert_bow([cleaned_reviews[i] for i in test_indices])\n",
    "train_y = cleaned_ratings[train_indices]\n",
    "test_y = cleaned_ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "40149f13-a0bc-4142-bed6-960add115585",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 24 # number of topics\n",
    "V = len(vocabulary_dict) # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "561f59c7-95a9-450d-9604-9478ba22011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the optimized global parameters from the model training phase\n",
    "np.random.seed(12345)\n",
    "new_alpha = pickle.load(open(\"data/scaledata/stochastic_K_{}/alpha.pickle\".format(K), \"rb\"))\n",
    "new_xi = pickle.load(open(\"data/scaledata/stochastic_K_{}/xi.pickle\".format(K), \"rb\"))\n",
    "new_eta = pickle.load(open(\"data/scaledata/stochastic_K_{}/eta.pickle\".format(K), \"rb\"))\n",
    "new_delta = pickle.load(open(\"data/scaledata/stochastic_K_{}/delta.pickle\".format(K), \"rb\"))\n",
    "new_Lambda = pickle.load(open(\"data/scaledata/stochastic_K_{}/Lambda.pickle\".format(K), \"rb\"))\n",
    "input_data_x = test_bow\n",
    "input_data_y = test_y\n",
    "fpath = \"fragmented_output_files_new/\" # where to store the temporary fragmented files during parallelized E steps\n",
    "if not os.path.exists(fpath[:-1]):\n",
    "    os.makedirs(fpath[:-1])\n",
    "else:\n",
    "    for fn in glob.glob(fpath + \"*\"):\n",
    "        os.remove(fn)\n",
    "epsilon = 1e-4 # stopping criteria for convergence in E step\n",
    "predict = True # prediction mode for evaluating the test set\n",
    "\n",
    "## Run batch mode variational EM\n",
    "elbo_vs_time = [-math.inf]\n",
    "improve_in_elbo = math.inf\n",
    "time_elapsed = 0\n",
    "    \n",
    "## Run one iteration of unsupervised E step (parallelized) to identify the optimal local variational parameters for all documents in the test set.\n",
    "## The optimized phi identified from the unsupervised batch VI will be used to generate the predictions for response variable y in the test set\n",
    "%run -i \"parallelized_sLDA_E_step.py\" # unsupervised batch VI for sLDA is way faster than supervised one\n",
    "all_gamma = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"gamma*\")]\n",
    "new_gamma_dict = merge_dict(all_gamma)\n",
    "new_gamma = create_gamma_matrix(new_gamma_dict)\n",
    "all_phi = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"phi*\")]\n",
    "new_phi = merge_dict(all_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90cdd765-3c3a-4772-a12e-23d27b57b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['last', 'screen', 'thing', 'place', 'again', 'actor', 'emotional', 'society', 'visual', 'manner'] 0.3213619743745454\n",
      "Topic 2: ['movies', 'world', 'sex', 'humor', 'why', 'bit', 'comes', 'where', 'seem', 'full'] 0.2897512105760993\n",
      "Topic 3: ['line', 'subject', 'subscribe', 'details', 'except', 'men', 'williams', 'act', 'word', 'elements'] 0.7148380566675236\n",
      "Topic 4: ['day', 'thin', 'red', 'songs', 'parker', 'offering', 'proves', 'gorgeous', 'wife', 'arnold'] 0.7726085416321286\n",
      "Topic 5: ['everything', 'leave', 'narrative', 'away', 'go', 'half', 'despite', 'along', 'adult', 'house'] 0.6200140509677692\n",
      "Topic 6: ['action', 'star', 'big', 'takes', 'human', 'under', 'watching', 'lines', 'moments', 'guy'] 0.32532140140097293\n",
      "Topic 7: ['kids', 'seems', 'motion', 'actors', 'once', 'rating', 'far', 'pg', 'screenplay', 'however'] 0.8174604161321043\n",
      "Topic 8: ['age', 'three', 'use', 'version', 'production', 'original', 'sort', 'words', 'producers', 'cinematic'] 0.8801950337215088\n",
      "Topic 9: ['plot', 'scene', 'seen', 'funny', 've', 'teenagers', 'someone', 'war', 'probably', 'almost'] 0.664638130101248\n",
      "Topic 10: ['high', 'romantic', 'find', 'woman', 'school', 'comedies', 'manages', 'start', 'concerned', 'bill'] 0.43471473943077776\n",
      "Topic 11: ['think', 'then', 'things', 'down', 'written', 'excellent', 'parts', 'turns', 'end', 'sometimes'] 0.5204979052940961\n",
      "Topic 12: ['though', 'sense', 'through', 'ever', 'family', 'simple', 'difficult', 'feels', 'message', 'beauty'] 0.5006954556107863\n",
      "Topic 13: ['bad', 'same', 'play', 'interesting', 'liked', 'king', 'worse', 'effect', 'close', 'lots'] 0.6089594040221786\n",
      "Topic 14: ['comedy', 'him', 'fun', 'de', 'mostly', 'take', 'mr', 'run', 'hardly', 'branagh'] 0.3066880985149566\n",
      "Topic 15: ['man', 'becomes', 'perhaps', 'back', 'room', 'seemed', 'idea', 'reply', 'until', 'trying'] 0.41323953506309674\n",
      "Topic 16: ['here', 'lot', 'ending', 'whose', 'gives', 'genre', 'offers', 'level', 'genuine', 'peter'] 0.37230499461462124\n",
      "Topic 17: ['after', 'us', 'material', 'totally', 'recommend', 'easy', 'waste', 'mine', 'opinions', 'expressed'] 0.788325043016122\n",
      "Topic 18: ['love', 'give', 'own', 'thought', 'certain', 'said', 'rather', 'michael', 'before', 'brief'] 0.7712413293933297\n",
      "Topic 19: ['year', 'acting', 'violence', 'john', 'long', 'another', 'great', 'runs', 'going', 'money'] 0.639849993781333\n",
      "Topic 20: ['every', 'music', 'american', 'come', 'familiar', 'years', 'moment', 'home', 'past', 'filmmaker'] 0.9868882756305056\n",
      "Topic 21: ['fact', 'perfect', 'keep', 'james', 'david', 'impressive', 'disney', 'main', 'impact', 'young'] 0.783762553808212\n",
      "Topic 22: ['both', 're', 'role', 'real', 'plays', 'style', 'right', 'less', 'tone', 'actually'] 0.8209726457863525\n",
      "Topic 23: ['minutes', 'special', 'over', 'performances', 'ca', 'quite', 'become', 'either', 'cinematography', 'number'] 0.33742067001209874\n",
      "Topic 24: ['really', 'hollywood', 'old', 'new', 'gets', 's', 'viewer', 'length', 'city', 'especially'] 0.6001927709382552\n"
     ]
    }
   ],
   "source": [
    "# Topic words of each topic learned from training sLDA, with corresponding coefficients (eta)\n",
    "inverse_vocabulary_dict = {v:k for k,v in vocabulary_dict.items()}\n",
    "for t in range(K):\n",
    "    word_indices = np.argsort(new_Lambda[t,:])[::-1][:10]\n",
    "    print(\"Topic {}:\".format(t+1), [inverse_vocabulary_dict[j] for j in word_indices], new_eta[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c45d5b0f-95b9-472c-a1bc-39f40f6d6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the response variable y for the test set:\n",
    "# For Gaussian response, y_hat = E[phi_bar^T eta]\n",
    "phi_bar = {k:v.mean(axis=0) for k,v in new_phi.items()}\n",
    "pred_y = np.empty((len(test_y),))\n",
    "for indx in range(len(test_y)):\n",
    "    pred_y[indx] = np.dot(phi_bar[indx], new_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "558d8248-ebd2-4fd2-ba76-56c96df57198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005427416040287403"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictive R^2\n",
    "def predictive_R2(test_y, pred_y):\n",
    "    return 1 - np.mean((test_y - pred_y)**2) / np.var(test_y)\n",
    "predictive_R2(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0382008-ca04-4b02-b4a7-4b60a609107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.03937403962004031, pvalue=0.21302585930320717)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pearson correlation coefficient\n",
    "pearsonr(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ced15-82da-4ae0-8fb5-69c9e0b1b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
