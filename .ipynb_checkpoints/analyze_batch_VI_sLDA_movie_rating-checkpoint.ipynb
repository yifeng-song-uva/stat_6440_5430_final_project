{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c064465-1463-42ba-80dc-5835fe1e6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from text_processing_utils import *\n",
    "import glob\n",
    "import math\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d79fc90-8c61-4229-862f-33995aa21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ratings = np.array(pickle.load(open(\"data/scaledata/cleaned_ratings.pickle\", \"rb\")))\n",
    "cleaned_reviews = pickle.load(open(\"data/scaledata/cleaned_reviews.pickle\", \"rb\"))\n",
    "vocabulary_dict = pickle.load(open(\"data/scaledata/vocabulary_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ddd8d24-7f82-4d25-b823-e790274b0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004 1002\n"
     ]
    }
   ],
   "source": [
    "# randomly split the movie reviews data into training/testing parts (80:20)\n",
    "np.random.seed(7654321)\n",
    "train_indices = np.random.choice(np.arange(len(cleaned_ratings)), int(len(cleaned_ratings)*0.8), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(cleaned_ratings)), train_indices)\n",
    "print(len(train_indices), len(test_indices))\n",
    "train_bow = convert_bow([cleaned_reviews[i] for i in train_indices])\n",
    "test_bow = convert_bow([cleaned_reviews[i] for i in test_indices])\n",
    "train_y = cleaned_ratings[train_indices]\n",
    "test_y = cleaned_ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40149f13-a0bc-4142-bed6-960add115585",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 24 # number of topics\n",
    "V = len(vocabulary_dict) # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "561f59c7-95a9-450d-9604-9478ba22011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the optimized global parameters from the model training phase\n",
    "np.random.seed(12345)\n",
    "new_alpha = pickle.load(open(\"data/scaledata/K_{}/alpha.pickle\".format(K), \"rb\"))\n",
    "new_xi = pickle.load(open(\"data/scaledata/K_{}/xi.pickle\".format(K), \"rb\"))\n",
    "new_eta = pickle.load(open(\"data/scaledata/K_{}/eta.pickle\".format(K), \"rb\"))\n",
    "new_delta = pickle.load(open(\"data/scaledata/K_{}/delta.pickle\".format(K), \"rb\"))\n",
    "new_Lambda = pickle.load(open(\"data/scaledata/K_{}/Lambda.pickle\".format(K), \"rb\"))\n",
    "input_data_x = test_bow\n",
    "input_data_y = test_y\n",
    "fpath = \"fragmented_output_files/\" # where to store the temporary fragmented files during parallelized E steps\n",
    "if not os.path.exists(fpath[:-1]):\n",
    "    os.makedirs(fpath[:-1])\n",
    "else:\n",
    "    for fn in glob.glob(fpath + \"*\"):\n",
    "        os.remove(fn)\n",
    "epsilon = 1e-4 # stopping criteria for convergence in E step\n",
    "predict = True # prediction mode for evaluating the test set\n",
    "\n",
    "## Run batch mode variational EM\n",
    "elbo_vs_time = [-math.inf]\n",
    "improve_in_elbo = math.inf\n",
    "time_elapsed = 0\n",
    "    \n",
    "## Run one iteration of unsupervised E step (parallelized) to identify the optimal local variational parameters for all documents in the test set.\n",
    "## The optimized phi identified from the unsupervised batch VI will be used to generate the predictions for response variable y in the test set\n",
    "%run -i \"parallelized_sLDA_E_step.py\" # unsupervised batch VI for sLDA is way faster than supervised one\n",
    "all_gamma = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"gamma*\")]\n",
    "new_gamma_dict = merge_dict(all_gamma)\n",
    "new_gamma = create_gamma_matrix(new_gamma_dict)\n",
    "all_phi = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"phi*\")]\n",
    "new_phi = merge_dict(all_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90cdd765-3c3a-4772-a12e-23d27b57b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['place', 'emotional', 'visual', 'manner', 'means', 'subtle', 'society', 'development', 'deep', 'opportunity'] 1.148248910632116\n",
      "Topic 2: ['sex', 'humor', 'why', 'comes', 'bit', 'gives', 'full', 'seem', 'where', 'enjoyable'] 0.44478638849350105\n",
      "Topic 3: ['line', 'subject', 'men', 'subscribe', 'details', 'style', 'find', 'job', 'women', 'word'] 0.39799809975981404\n",
      "Topic 4: ['once', 'day', 'night', 'williams', 'opening', 'proves', 'wife', 'somewhere', 'chris', 'parker'] 0.11100095267374854\n",
      "Topic 5: ['through', 'take', 'everything', 'away', 'adult', 'novel', 'narrative', 'stories', 'relationship', 'yet'] 0.4847768859071735\n",
      "Topic 6: ['action', 'big', 'screen', 'star', 'moments', 'lines', 'watching', 'during', 'obvious', 'along'] 0.17964843444244138\n",
      "Topic 7: ['seems', 'motion', 'however', 'pictures', 'far', 'feature', 'art', 'woman', 'screenplay', 'supporting'] 1.1330934594665436\n",
      "Topic 8: ['three', 'effective', 'original', 'black', 'use', 'human', 'cinema', 'producers', 'approach', 's'] 1.4598076542228178\n",
      "Topic 9: ['funny', 'plot', 'scene', 'almost', 'teenagers', 've', 'language', 'since', 'seen', 'written'] 0.4666318455821621\n",
      "Topic 10: ['high', 'game', 'school', 'woody', 'debut', 'joe', 'wild', 'gay', 'ed', 'artist'] 0.2550909928840248\n",
      "Topic 11: ['still', 'know', 'excellent', 'movies', 'things', 'fun', 'romantic', 'anything', 'think', 'worth'] 0.14470931697561795\n",
      "Topic 12: ['though', 'sense', 'thing', 'family', 'message', 'heart', 'takes', 'power', 'clear', 'theme'] 1.0280546211443795\n",
      "Topic 13: ['bad', 'least', 'play', 'interesting', 'seeing', 'level', 'same', 'liked', 'summer', 'times'] -0.9304547332771946\n",
      "Topic 14: ['comedy', 'love', 'him', 'john', 'last', 'everyone', 'viewer', 'taking', 'comic', 'wants'] 0.5758004444473261\n",
      "Topic 15: ['us', 'man', 'someone', 'becomes', 'room', 'reply', 'situations', 'hard', 'screening', 'himself'] 0.5198236815974013\n",
      "Topic 16: ['here', 'actors', 'lot', 'ending', 'itself', 'whose', 'off', 'looking', 'likely', 'mystery'] 0.7171441850923514\n",
      "Topic 17: ['kids', 'back', 'totally', 'set', 'watch', 'waste', 'thriller', 'sometimes', 'recommend', 'r'] 0.7209624023569532\n",
      "Topic 18: ['give', 'want', 'before', 'probably', 'thought', 'each', 'after', 'point', 'rather', 'certainly'] 0.39833582852783067\n",
      "Topic 19: ['year', 'long', 'acting', 'violence', 'another', 'cast', 'go', 'must', 'great', 'poor'] 0.5486993536015063\n",
      "Topic 20: ['music', 'every', 'american', 'themes', 'years', 'experience', 'history', 'familiar', 'allen', 'powerful'] 1.216455141190183\n",
      "Topic 21: ['david', 'keep', 'james', 'tension', 'laugh', 'musical', 'leaves', 'genre', 'death', 'animated'] 0.7951934084458046\n",
      "Topic 22: ['re', 'both', 'real', 'role', 'ever', 'plays', 'tale', 'come', 'right', 'anyone'] 0.38351283744133013\n",
      "Topic 23: ['over', 'world', 'say', 'minutes', 'less', 'new', 'ca', 'performances', 'fact', 'cinematography'] 0.7225697175365049\n",
      "Topic 24: ['really', 'gets', 'hollywood', 'old', 'age', 'different', 'often', 'always', 'own', 'bond'] 1.2794367176869272\n"
     ]
    }
   ],
   "source": [
    "# Topic words of each topic learned from training sLDA, with corresponding coefficients (eta)\n",
    "inverse_vocabulary_dict = {v:k for k,v in vocabulary_dict.items()}\n",
    "for t in range(K):\n",
    "    word_indices = np.argsort(new_Lambda[t,:])[::-1][:10]\n",
    "    print(\"Topic {}:\".format(t+1), [inverse_vocabulary_dict[j] for j in word_indices], new_eta[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c45d5b0f-95b9-472c-a1bc-39f40f6d6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the response variable y for the test set:\n",
    "# For Gaussian response, y_hat = E[phi_bar^T eta]\n",
    "phi_bar = {k:v.mean(axis=0) for k,v in new_phi.items()}\n",
    "pred_y = np.empty((len(test_y),))\n",
    "for indx in range(len(test_y)):\n",
    "    pred_y[indx] = np.dot(phi_bar[indx], new_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "558d8248-ebd2-4fd2-ba76-56c96df57198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1612400711748151"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictive R^2\n",
    "def predictive_R2(test_y, pred_y):\n",
    "    return 1 - np.mean((test_y - pred_y)**2) / np.var(test_y)\n",
    "predictive_R2(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0382008-ca04-4b02-b4a7-4b60a609107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.40737329465495675, pvalue=2.4263262047789396e-41)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pearson correlation coefficient\n",
    "pearsonr(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ced15-82da-4ae0-8fb5-69c9e0b1b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
