{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d689ede8-189c-45e1-897b-f3e6b96bbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from text_processing_utils import *\n",
    "from variational_inference_utils import *\n",
    "import glob\n",
    "import math\n",
    "from variational_inference_sLDA_M_step import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c2c6b2-0b35-4692-b6b1-7acfeeef6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ratings = np.array(pickle.load(open(\"data/scaledata/cleaned_ratings.pickle\", \"rb\")))\n",
    "cleaned_reviews = pickle.load(open(\"data/scaledata/cleaned_reviews.pickle\", \"rb\"))\n",
    "vocabulary_dict = pickle.load(open(\"data/scaledata/vocabulary_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382e0a36-c282-47c7-8309-f9026ed56fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004 1002\n"
     ]
    }
   ],
   "source": [
    "# split the movie reviews data into training/testing parts (80:20)\n",
    "np.random.seed(54321)\n",
    "train_indices = np.random.choice(np.arange(len(cleaned_ratings)), int(len(cleaned_ratings)*0.8), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(cleaned_ratings)), train_indices)\n",
    "print(len(train_indices), len(test_indices))\n",
    "train_bow = convert_bow([cleaned_reviews[i] for i in train_indices])\n",
    "test_bow = convert_bow([cleaned_reviews[i] for i in test_indices])\n",
    "train_y = cleaned_ratings[train_indices]\n",
    "test_y = cleaned_ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbad692d-0daf-4ac2-b8e4-a5f562639f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save parameters to Google Drive\n",
    "output_dir = \"data/scaledata/stochastic_K_{}\".format(K)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "else:\n",
    "    delete_all_files(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541cd865-40ae-4b94-a114-a7828744445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mode variational EM iteration 1: elbo = -300934.3569049835\n",
      "Batch mode variational EM iteration 2: elbo = -208433.97087574005\n",
      "Batch mode variational EM iteration 3: elbo = -186011.03345251083\n",
      "Batch mode variational EM iteration 4: elbo = -177071.1480937004\n",
      "Batch mode variational EM iteration 5: elbo = -172514.37959241867\n",
      "Batch mode variational EM iteration 6: elbo = -169803.0082168579\n"
     ]
    }
   ],
   "source": [
    "K = 24 # number of topics\n",
    "V = len(vocabulary_dict) # vocabulary size\n",
    "fpath = \"fragmented_output_files/\" # where to store the temporary fragmented files during parallelized E steps\n",
    "if not os.path.exists(fpath[:-1]):\n",
    "    os.makedirs(fpath[:-1])\n",
    "else:\n",
    "    delete_all_files(fpath[:-1])\n",
    "    \n",
    "## Initialization\n",
    "np.random.seed(1234567)\n",
    "initial_batch_size = 1000\n",
    "sample_indices = np.random.choice(np.arange(len(train_bow)), initial_batch_size, replace=False)\n",
    "train_bow_sample = {new_i:train_bow[new_i] for new_i,i in enumerate(sample_indices)}\n",
    "train_y_sample = train_y[sample_indices]\n",
    "input_data_x = train_bow_sample\n",
    "input_data_y = train_y_sample\n",
    "predict = False\n",
    "np.random.seed(12345)\n",
    "new_alpha = np.array([1/K]*K)\n",
    "new_xi = np.array([1/V]*V)\n",
    "new_eta = np.linspace(-1,1,K)\n",
    "new_delta = np.var(train_y, ddof=1)\n",
    "new_Lambda = np.abs(np.random.normal(loc=0, scale=0.1, size=K*V)).reshape((K,V)) # initialize Lambda randomly (add a small half-normal distribution to 1)\n",
    "epsilon = 1e-4\n",
    "elbo_epsilon=0.1 # percentage\n",
    "\n",
    "## Run batch mode variational EM\n",
    "elbo_vs_time = [-math.inf]\n",
    "improve_in_elbo = math.inf\n",
    "j = 0\n",
    "while improve_in_elbo > elbo_epsilon:\n",
    "    ### Run one iteration of E step (parallelized)\n",
    "    %run -i \"parallelized_sLDA_E_step.py\"\n",
    "    all_gamma = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"gamma*\")]\n",
    "    new_gamma_dict = merge_dict(all_gamma)\n",
    "    new_gamma = create_gamma_matrix(new_gamma_dict)\n",
    "    all_phi = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"phi*\")]\n",
    "    new_phi = merge_dict(all_phi)\n",
    "    ### Run one iteration of M step\n",
    "    m_step = batch_VI_sLDA_M_Step(K, input_data_x, input_data_y,\n",
    "                                  new_alpha, new_xi, new_eta, new_delta, new_Lambda,\n",
    "                                  new_gamma, new_phi,\n",
    "                                  len(input_data_y), epsilon)\n",
    "    new_Lambda, new_alpha, new_xi, new_eta, new_delta, new_elbo = m_step.run()\n",
    "    improve_in_elbo = pct_diff(elbo_vs_time[-1], new_elbo)\n",
    "    elbo_vs_time.append(new_elbo)\n",
    "    j += 1\n",
    "    for var in ['Lambda', 'alpha', 'xi', 'eta', 'delta']:\n",
    "        pickle.dump(eval(\"new_\"+var), open(output_dir + \"/warmup_{0}_{1}.pickle\".format(var, check_points[t]), \"wb\")) \n",
    "    print(\"Batch mode variational EM iteration {}: elbo =\".format(j), new_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c455bd-f1d5-4de4-b41c-c3746f3d622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44969256 0.50547697 0.44973926 0.26830226 0.53737989 0.62371218\n",
      " 0.61031766 0.77913044 0.48074952 0.73049012 0.65799448 0.81802608]\n",
      "0.03199283682215594\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(654321)\n",
    "delete_all_files(fpath[:-1])\n",
    "## Run minibatch (stochastic) mode variational EM\n",
    "S = 500 # minibatch size\n",
    "n_iter_batch = len(pickle.load(open(\"/content/drive/MyDrive/batch_VI_sLDA_movie_rating/K_{}/elbo_vs_time.pickle\".format(K), \"rb\")))\n",
    "n_iter = int((n_iter_batch+1) * len(train_bow) / S) # total # of iterations of minibatch variational EM: equivalent to # of full passes of training data needed for the batch variational EM to converge\n",
    "temp = S * np.arange(1,n_iter+1) // len(train_bow)\n",
    "check_points = [np.arange(1,n_iter+1)[temp == i][0] for i in range(1, n_iter_batch+1)]  # minibatch iterations that correspond to each iteration in batch mode\n",
    "switch_point = check_points[int(j * initial_batch_size / S)] + 1    ## save parameters to Google Drive\n",
    "check_points = np.array(check_points)\n",
    "check_points = check_points[check_points >= switch_point]\n",
    "check_points = {val:(i+1) for i,val in enumerate(check_points)}\n",
    "kappa = 0.6 # \"forgetting rate\"\n",
    "tau = 1 # \"delay\"\n",
    "\n",
    "for t in range(switch_point, n_iter+1):\n",
    "\n",
    "    ### randomly sample a minibatch with size S\n",
    "    sample_indices = np.random.choice(np.arange(len(train_bow)), S, replace=False)\n",
    "    train_bow_sample = {new_i:train_bow[new_i] for new_i,i in enumerate(sample_indices)}\n",
    "    train_y_sample = train_y[sample_indices]\n",
    "    input_data_x = train_bow_sample\n",
    "    input_data_y = train_y_sample\n",
    "    \n",
    "    ### Run one iteration of E step (parallelized)\n",
    "    %run -i \"parallelized_sLDA_E_step.py\"\n",
    "    all_gamma = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"gamma*\")]\n",
    "    new_gamma_dict = merge_dict(all_gamma)\n",
    "    new_gamma = create_gamma_matrix(new_gamma_dict)\n",
    "    all_phi = [pickle.load(open(fn, \"rb\")) for fn in glob.glob(fpath + \"phi*\")]\n",
    "    new_phi = merge_dict(all_phi)\n",
    "\n",
    "    ### Run one iteration of M step\n",
    "    rho = step_size(t,tau,kappa) # rho_t\n",
    "    m_step = VI_sLDA_M_Step(K, train_bow_sample, train_y_sample,\n",
    "                            new_alpha, new_xi, new_eta, new_delta, new_Lambda,\n",
    "                            new_gamma, new_phi,\n",
    "                            len(train_bow), rho)\n",
    "    new_Lambda, new_alpha, new_xi, new_eta, new_delta = m_step.run()\n",
    "    print(new_eta)\n",
    "    print(new_delta)\n",
    "    if t in check_points:\n",
    "        for var in ['Lambda', 'alpha', 'xi', 'eta', 'delta']:\n",
    "            pickle.dump(eval(\"new_\"+var), open(output_dir + \"/{0}_{1}.pickle\".format(var, check_points[t]), \"wb\"))        \n",
    "    print(\"Complete minibatch variational EM iteration {}!\".format(t-switch_point+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63d9697e-7e14-4d6b-8041-204a89439dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_bar_times_y = np.dot(m_step.y, m_step.phi_bar) # K-dimensional vector\n",
    "expect_x_x_t_times_eta = np.dot(m_step.expect_x_x_t, m_step.eta) # K-dimensional vector\n",
    "y_t_y = np.sum(m_step.y**2)\n",
    "temp_var = np.dot(m_step.eta, phi_bar_times_y - expect_x_x_t_times_eta/2) # dot product\n",
    "g_eta = (1/m_step.delta)*(phi_bar_times_y - expect_x_x_t_times_eta) # K-dimensional vector\n",
    "g_delta = -m_step.D/2/m_step.delta + 1/2/m_step.delta**2 * (y_t_y - 2*temp_var)\n",
    "g = m_step.scale_factor * np.hstack([g_eta, np.array([g_delta])]) # gradient is of K+1 dimensional, scale based on minibatch size\n",
    "h_11 = -m_step.expect_x_x_t/m_step.delta\n",
    "h_21 = -g_eta / m_step.delta # mixed partial derivatives: K-dimensional vector\n",
    "h_22 = m_step.D/2/m_step.delta**2 - 1/m_step.delta**3 * (y_t_y - 2*temp_var)\n",
    "h = np.zeros(shape=(m_step.K+1, m_step.K+1)) # Hessian of L w.r.t (eta, delta)\n",
    "h[:m_step.K, :m_step.K] = h_11\n",
    "h[m_step.K, m_step.K] = h_22\n",
    "h[m_step.K, :m_step.K] = h_21\n",
    "h[:m_step.K, m_step.K] = h_21\n",
    "h = m_step.scale_factor * h # (scaled) Hessian is of (K+1) x (K+1) dimensional\n",
    "h_inv = np.linalg.inv(h)\n",
    "eta_delta_hat = h_inv @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddbebd-e3cc-47e9-aaf7-3180757c2a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
